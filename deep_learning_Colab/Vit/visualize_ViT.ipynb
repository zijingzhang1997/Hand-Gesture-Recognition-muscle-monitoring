{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZII5pk69M1cG"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib\n","import torch\n","# !pip install mat73\n","# import mat73\n","import matplotlib.pyplot as plt\n","import csv\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split,TensorDataset\n","from torchvision import transforms, utils\n","import time\n","import pandas as pd\n","import scipy.io\n","import sklearn.metrics\n","import seaborn as sns\n","import random\n","from sklearn.model_selection import train_test_split\n","from torchvision.transforms import ToTensor\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import accuracy_score\n","random.seed(1)\n","torch.manual_seed(1)\n","torch.cuda.manual_seed(1)\n","np.random.seed(1)\n","\n","from scipy import signal\n","\n","from sklearn.metrics import confusion_matrix\n","\n","import seaborn as sn\n","import pandas as pd\n","\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.model_selection import LeaveOneGroupOut\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4126,"status":"ok","timestamp":1646943306882,"user":{"displayName":"Zijing Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOGqelSoQ8DBz00Gsbqr9QBAymTf0Nlug7AzpX=s64","userId":"03111893666314698246"},"user_tz":300},"id":"WbbJyKP4Y8zU","outputId":"4ec12477-5219-4b87-c7ce-184cdcd28695"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n"]}],"source":["from math import sqrt\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","!pip install einops\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# helpers\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# classes\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class LSA(nn.Module):\n","    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n","        super().__init__()\n","        inner_dim = dim_head *  heads\n","        self.heads = heads\n","        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n","\n","        self.attend = nn.Softmax(dim = -1)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n","\n","        dots = torch.matmul(q, k.transpose(-1, -2)) * self.temperature.exp()\n","\n","        mask = torch.eye(dots.shape[-1], device = dots.device, dtype = torch.bool)\n","        mask_value = -torch.finfo(dots.dtype).max\n","        dots = dots.masked_fill(mask, mask_value)\n","\n","        attn = self.attend(dots)\n","\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        return self.to_out(out)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, LSA(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","class SPT(nn.Module):\n","    def __init__(self, *, dim, patch_size, channels = 3):\n","        super().__init__()\n","        patch_dim = patch_size * patch_size * 5 * channels\n","\n","        self.to_patch_tokens = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n","            nn.LayerNorm(patch_dim),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","    def forward(self, x):\n","        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n","        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n","        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n","        return self.to_patch_tokens(x_with_shifts)\n","\n","class ViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = SPT(dim = dim, patch_size = patch_size, channels = channels)\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","        x = self.dropout(x)\n","\n","        x = self.transformer(x)\n","\n","        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","\n","        x = self.to_latent(x)\n","        return self.mlp_head(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89744,"status":"ok","timestamp":1646943454009,"user":{"displayName":"Zijing Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOGqelSoQ8DBz00Gsbqr9QBAymTf0Nlug7AzpX=s64","userId":"03111893666314698246"},"user_tz":300},"id":"FQKGrSY6KDlv","outputId":"e8dcfcbc-11ee-4bda-eb7e-331929f20ae2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","FiltName='filt_0.1_5'\n","file_name_data=r\"/content/drive/My Drive/Colab Notebooks/RFMG/data/data_all_per8_multi6_\"+FiltName+\".npz\"\n","data=np.load(file_name_data)\n","\n","# feature_cwt_per=data['feature_cwt_per']\n","feature_stft_per=data['feature_stft_per']\n","feature_stft2_per=data['feature_stft2_per']\n","\n","feature_cwt_per=data['feature_cwt_per']\n","feature_cwt2_per=data['feature_cwt2_per']\n","feature_cwt3_per=data['feature_cwt3_per']\n","\n","label_all_per=data['label_all_per']\n","caseNum_all_per=data['caseNum_all_per']\n","perNum_all_per=data['perNum_all_per']\n","\n","data=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgrJAtXbYyoc"},"outputs":[],"source":["def Load_TransModel(FeatType,person,opt):\n","\n","\n","  dataPath=\"/content/drive/My Drive/Colab Notebooks/RFMG/data/\"\n","  FeatName=['stft','stft2','cwt','cwt2','cwt3'] \n","  \n","  PATH=dataPath+\"/model_all_vit/\"+opt['modelVer']+\"/\"+FeatName[FeatType]+\"_ex_\"+str(person)+\".pt\"\n","  model_trans = ViT(\n","        image_size =opt['image_size'],\n","        patch_size = opt['patch_size'],\n","        num_classes =num_class,\n","        dim = par_vit['dim'],\n","        depth = par_vit['depth'],\n","        heads = par_vit['heads'],\n","        mlp_dim = par_vit['mlp_dim'],\n","        dropout = par_vit['dropout'],\n","        emb_dropout = par_vit['emb_dropout'],\n","        channels = in_ch\n","        ).cuda()\n","  model_trans.load_state_dict(torch.load(PATH))\n","  model_trans.eval()\n","\n","  # for param in model_trans.parameters():\n","  #     param.requires_grad = False\n","\n","  # model_trans.fc1 = nn.Linear(model_trans.fc1.in_features, model_trans.fc2.in_features)\n","  # model_trans.fc2 = nn.Linear(model_trans.fc2.in_features, num_class)\n","  return model_trans\n","in_ch=48\n","num_class=23\n","mVer='v1_per8'\n","par_vit={'dim':512,'depth':6,'heads':16,'mlp_dim':32,'dropout':0.1,'emb_dropout':0.1};\n","opt1={'image_size':56,'patch_size':7,'modelVer':mVer}\n","opt2={'image_size':40,'patch_size':5,'modelVer':mVer}\n","opt3={'image_size':125,'patch_size':5,'modelVer':mVer}\n","model_trans=Load_TransModel(3,1,opt3)\n","\n"]},{"cell_type":"code","source":["print(model_trans)\n","def get_attention_map(img, get_mask=False):\n","    \n","    x.size()\n","\n","    logits, att_mat = model(x.unsqueeze(0))\n","\n","    att_mat = torch.stack(att_mat).squeeze(1)\n","\n","    # Average the attention weights across all heads.\n","    att_mat = torch.mean(att_mat, dim=1)\n","\n","    # To account for residual connections, we add an identity matrix to the\n","    # attention matrix and re-normalize the weights.\n","    residual_att = torch.eye(att_mat.size(1))\n","    aug_att_mat = att_mat + residual_att\n","    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n","\n","    # Recursively multiply the weight matrices\n","    joint_attentions = torch.zeros(aug_att_mat.size())\n","    joint_attentions[0] = aug_att_mat[0]\n","\n","    for n in range(1, aug_att_mat.size(0)):\n","        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n","\n","    v = joint_attentions[-1]\n","    grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n","    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n","    if get_mask:\n","        result = cv2.resize(mask / mask.max(), img.size)\n","    else:        \n","        mask = cv2.resize(mask / mask.max(), img.size)[..., np.newaxis]\n","        result = (mask * img).astype(\"uint8\")\n","    \n","    return result"],"metadata":{"id":"ozg1UWqmfXTJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hv19s1FdufVK"},"outputs":[],"source":["in_ch=48\n","num_class=23\n","self_channel=[0,1,10,11,20,21,30,31]\n","par_vit1={'dim':512,'depth':6,'heads':16,'mlp_dim':32,'dropout':0.1,'emb_dropout':0.1};\n","par_vit2={'dim':1024,'depth':6,'heads':16,'mlp_dim':32,'dropout':0.1,'emb_dropout':0.1};\n","par_vit=par_vit1\n","parameter_vit=np.array(list(par_vit.items()));\n","\n","def test_trans(X_all,y_all,opt,k_split,n_sel,trans_net):\n","  import torch\n","  torch.cuda.empty_cache()\n","\n","  if __name__ == '__main__':\n","      \n","      transform = ToTensor()\n","      batchsize_train = 16\n","\n","      folds=RepeatedStratifiedKFold(n_splits=k_split, n_repeats=1,random_state=100).split(X_all,y_all)\n","\n","      test_ind_all=[]\n","      train_ind_all=[]\n","      for train_index, test_index in folds:\n","        test_ind_all.append(test_index)\n","        train_ind_all.append(train_index)\n","      \n","\n","      # train_ind=train_ind_all[n_sel]\n","      # test_ind=test_ind_all[n_sel]\n","\n","      # swap train & test index \n","      # select one fold to be trained by n_sel\n","      train_ind=test_ind_all[n_sel]\n","      test_ind=train_ind_all[n_sel]\n","\n","\n","      X_train=X_all[train_ind] \n","      X_test=X_all[test_ind]\n","      y_train=y_all[train_ind] \n","      y_test=y_all[test_ind]\n","\n","      train_loader = DataLoader(TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train)), batch_size=batchsize_train,shuffle=True)\n","      batchsize_test = 16\n","      test_loader = DataLoader(TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test)), batch_size=batchsize_test, shuffle=False)\n","\n","\n","  \n","      momentum = 0.1\n","      random_seed=1\n","      torch.backends.cudnn.enabled = False\n","      torch.manual_seed(random_seed)     #设定随机数种子为固定值\n","      \n","      train_loss_epoch = []\n","      test_acc_epoch =[]\n","      total_acc_epoch = []\n","      #epoch_range = np.arange(25,225,25)\n","      epoch_range = np.array([2])\n","\n","   \n","      if  opt['trans']==0:\n","        network = ViT(\n","        image_size = opt['image_size'],\n","        patch_size = opt['patch_size'],\n","        num_classes =num_class,\n","        dim = par_vit['dim'],\n","        depth = par_vit['depth'],\n","        heads = par_vit['heads'],\n","        mlp_dim = par_vit['mlp_dim'],\n","        dropout = par_vit['dropout'],\n","        emb_dropout = par_vit['emb_dropout'],\n","        channels = in_ch\n","        ).cuda()\n","      if opt['trans']==1:\n","        network =trans_net.cuda()\n","      #training\n","      learning_rate=opt['learning rate']\n","      optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n","      #optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.9)\n","      \n","\n","      #network.train()\n","      Training_Loss = []\n","      Test_Loss = []\n","      start_time = time.time()\n","      criterion = nn.CrossEntropyLoss()\n","      #criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1, 3]).cuda())\n","      for epoch in range(opt['n_epoch']):   # loop over the dataset multiple times\n","          train_loss = 0\n","          for X, Y in train_loader:\n","              # X = X.view(-1,1,X.shape[2],X.shape[3]).cuda()\n","              X = X.float().cuda()  \n","              Y = Y.long().view(-1, ).cuda() \n","              current_batchsize = X.shape[0]\n","              optimizer.zero_grad()\n","              output = network(X)\n","              loss = criterion(output,Y)\n","              train_loss = train_loss + loss.item()\n","              loss.backward()                     #calculate the gradient decent\n","              optimizer.step()                    #update the weight\n","              \n","            \n","          test_loss = 0\n","          correct = 0\n","          total = 0\n","          test_y= []\n","          test_y_p = []\n","     \n","          with torch.no_grad():        \n","              # X = X.view(-1,X.shape[1],X.shape[2])\n","              # X = X.float()         \n","              for X, Y in test_loader:\n","                  X = torch.Tensor(X).cuda()\n","                  Y = torch.Tensor(Y).long().view(-1, ).cuda()\n","                  images, labels = X, Y\n","                  # calculate outputs by running images through the network\n","                  outputs = network(images)\n","                  loss = criterion(outputs,Y)\n","                  test_loss = test_loss + loss.item()\n","\n","                  # the class with the highest energy is what we choose as prediction\n","                  _, predicted = torch.max(outputs.data, 1)\n","                  total += labels.size(0)\n","                  correct += (predicted == labels).sum().item()\n","                  \n","                  for i in range(len(labels)):\n","                    test_y.append(labels[i])\n","                    test_y_p.append(predicted[i])\n","          \n","          #print('Accuracy of test cases: %d %%' % (100 * correct / total))\n","  \n","\n","          Training_Loss.append(train_loss/len(train_loader.dataset))\n","          Test_Loss.append(test_loss/len(test_loader.dataset))\n","   \n","\n","  test_y_p=torch.FloatTensor(test_y_p)\n","  test_y=torch.FloatTensor(test_y)\n","  test_y_p=np.array(test_y_p.cpu())\n","  test_y=np.array(test_y.cpu())\n","  test_y2=np.stack((test_y, test_y_p))\n","  \n","  train_ind_inAll=train_ind\n","  test_ind_inAll=test_ind\n","\n","  trn_tst_ind=[train_ind_inAll,test_ind_inAll]\n","\n","  cm=confusion_matrix(test_y, test_y_p)\n","  acc=accuracy_score(test_y, test_y_p)\n","  print(cm,acc)\n","  return cm,acc,test_y2,train_ind_inAll,test_ind_inAll\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXlRIaYjoQC_"},"outputs":[],"source":["Folder='model_all_vit/v1_per8_trans_result/'\n","\n","def plotfigFullClass(cm,acc,tle,per):\n","\n","    parent_dir = r\"/content/drive/My Drive/Colab Notebooks/RFMG/data/\"+Folder+str(per)+'/'\n","    path1 = os.path.join(parent_dir, 'cm_fig')\n","    os.makedirs(path1, exist_ok = True) \n","    path1 = os.path.join(parent_dir, 'npyFile')\n","    os.makedirs(path1, exist_ok = True) \n","\n","\n","\n","    labelNum=str(1)  #labelling method 1,2,3,4\n","    size=15\n","    name1=[\"R\", \"G\",\"G*2\",\"P1\",\"P1*2\",\"P2\",\"P2*2\",\"P23\",\"P23*2\",\"P4\",\"P4*2\",\"sG\",\"sF\",\"sP1\",\"sP2\",\"sP23\",\"sP4\",\"U\",\"U*2\",\"D\",\"D*2\",\"sU\",\"sD\"]\n","    name2=name1\n","    w=20 #fig size 1      change when label num change\n","    h=22   #fig size 2\n","    \n","    # true_num=0\n","    # for i in range (len(cm)):\n","    #   true_num=true_num+ cm[i,i]\n","    # acc=true_num/np.sum(cm)   \n","    \n","    df_cm = pd.DataFrame(cm, index=name1, columns=name2)\n","    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n","    plt.figure()\n","    sn.set(font_scale=1.2)\n","    mask = np.zeros_like(df_cm)\n","    mask[np.where(cm==0)] = True\n","    \n","    sn.heatmap(df_cm, cmap=\"YlGnBu\",fmt=\".2f\",vmin=0, vmax=1.0,annot=True,mask=mask,square=True,cbar=False,annot_kws={\"size\": size,'fontsize':15})\n"," \n","    s01=' Acc={n:.3f}'.format(n=acc)\n","    #plt.text(a,b,s01,fontsize=size)\n","    \n","    tleSave=tle\n","    plt.title(tle+s01)\n","\n","    figure = plt.gcf()\n","    figure.set_size_inches(w, h)\n","    \n","    plt.savefig(parent_dir+r\"cm_fig/\"+tleSave+\".png\", dpi=300)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYciIcz7MDgu"},"outputs":[],"source":["mVer='v1_per8'\n","opt1={'image_size':56,'patch_size':7,'modelVer':mVer}\n","opt2={'image_size':40,'patch_size':5,'modelVer':mVer}\n","opt3={'image_size':125,'patch_size':5,'modelVer':mVer}\n","## for testing kfold CV transfer option\n","n1=40\n","#lr1=1e-5 #  old version. accuracy is lower for participants with higher accuracy, more similarity \n","lr1=1e-4  # for participant except 1 \n","#lr1=2e-4  # for participant 1 \n","opt1_t1={'image_size':56,'patch_size':7,'trans':1,'learning rate':lr1,'n_epoch':n1}\n","opt2_t1={'image_size':40,'patch_size':5,'trans':1,'learning rate':lr1,'n_epoch':n1}\n","opt3_t1={'image_size':125,'patch_size':5,'trans':1,'learning rate':lr1,'n_epoch':n1}\n","## for testing kfold CV not transfer option\n","n2=20\n","opt1_t0={'image_size':56,'patch_size':7,'trans':0,'learning rate':1e-4,'n_epoch':n2}\n","opt2_t0={'image_size':40,'patch_size':5,'trans':0,'learning rate':1e-4,'n_epoch':n2}\n","opt3_t0={'image_size':125,'patch_size':5,'trans':0,'learning rate':1e-4,'n_epoch':n2}\n","opt_list=[opt1,opt2,opt3,opt3,opt3]\n","opt_t1_list=[opt1_t1,opt2_t1,opt3_t1,opt3_t1,opt3_t1]\n","opt_t0_list=[opt1_t0,opt2_t0,opt3_t0,opt3_t0,opt3_t0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1Qy15WGQFlr"},"outputs":[],"source":["\n","# use one feature ver to test \n","# (p,k_split,transOpt) p participant number 1~8\n","# k_split, one \n","feature_list=[feature_stft_per,feature_stft2_per,feature_cwt_per,feature_cwt2_per,feature_cwt3_per]\n","def main_trans_CV1(p,k_split,transOpt,FeatOpt):  \n","\n","    \n","\n","\n","    channel=np.linspace(0,47,48).astype(int) \n","    in_ch=48\n","    num_class=23\n","   \n","    cm=[]\n","    cm_norm=[]\n","    test_y_pred=[]\n","    test_y_true=[]\n","    acc=[]\n","    acc_all=[]\n","    trn_ind=[]\n","    tst_ind=[]\n","\n","    \n","    feature_listName=[\"stft\",\"stft2\",\"cwt\",\"cwt2\",\"cwt3\"]\n","    if __name__ == '__main__':\n","      \n","        l=FeatOpt  # feature list index. \"stft\",\"stft2\",\"cwt\",\"cwt2\",\"cwt3\"\n","  \n","        p_test=p  # test participant's number \n","\n","       # k_split=3 # k-fold  1/n train \n","       # n_sel=0 \n","\n","        ind=np.where(perNum_all_per==p_test)[0]   # participant index for whole kulti dataset \n","        #model_trans=Load_TransModel(l,p_test,opt_list[l]) #  (FeatType,person,opt):     FeatType: ['stft','stft2','cwt','cwt2','cwt3'] \n","\n","        for j in range(k_split):\n","          if transOpt==1:\n","            opt_temp=opt_t1_list[l]\n","          if transOpt==0:\n","            opt_temp=opt_t0_list[l]\n","          opt_temp_arr=np.array(list(opt_temp.items()));\n","\n","          model_trans=Load_TransModel(l,p_test,opt_list[l])\n","          print()\n","          cm_temp,acc_temp,test_y_temp,trn_ind_temp,tst_ind_temp=test_trans(feature_list[l][ind,:,:,:],label_all_per[ind],opt_temp,k_split,j,model_trans)\n","  \n","    \n","          cm.append(cm_temp)\n","          acc_all.append(acc_temp)\n","          \n","          trn_ind.append(trn_ind_temp)\n","          tst_ind.append(tst_ind_temp)\n","\n","      \n","\n","          for k in range(len(tst_ind_temp)):\n","            test_y_pred.append(test_y_temp[1,k])\n","            test_y_true.append(test_y_temp[0,k])\n","  \n","        acc_all=np.array(acc_all)\n","        test_y_pred=np.array(test_y_pred)\n","        test_y_true=np.array(test_y_true)\n","        cm=confusion_matrix(test_y_pred, test_y_true)\n","        cm_norm=confusion_matrix(test_y_pred, test_y_true,normalize='true')\n","        acc=accuracy_score(test_y_pred, test_y_true)\n","\n","   \n","    SaveName='ch'+str(in_ch)+'_'+str(k_split)+'fold_'+feature_listName[l]+'_trans_'+str(transOpt)\n","  \n","    print(cm_norm.shape)\n","    print(test_y_pred.shape)\n","    plotfigFullClass(cm_norm,acc,SaveName,p)\n"," \n","    np.savez_compressed(\"/content/drive/My Drive/Colab Notebooks/RFMG/data/\"+Folder+str(p)+\"/npyFile/\"+SaveName+\".npz\",\\\n","                    cm_norm=cm_norm,cm=cm,acc=acc,acc_all=acc_all,\\\n","                    test_y=np.stack((test_y_pred, test_y_true)),test_ind=np.array(tst_ind),\\\n","                   parameter_vit=parameter_vit,opt=opt_temp_arr)\n","    \n","# main_trans_CV1(p,k_split,transOpt,FeatOpt)   \n","\n","# for pp in range(1,9):\n","#   main_trans_CV1(pp,4,1,3)\n","#   main_trans_CV1(pp,4,0,3)\n","# for pp in range(1,9):\n","#   main_trans_CV1(pp,3,1,3)\n","#   main_trans_CV1(pp,3,0,3)\n","for vv in range(5): # feat version 0~4\n","  for pp in range(1,9): # participant number label 1~8 \n","    for k in [4]:\n","      main_trans_CV1(pp,k,1,vv)\n","      #main_trans_CV1(pp,k,0,vv)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLKnTNAbUEdi"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1386,"status":"ok","timestamp":1645465580309,"user":{"displayName":"Zijing Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOGqelSoQ8DBz00Gsbqr9QBAymTf0Nlug7AzpX=s64","userId":"03111893666314698246"},"user_tz":300},"id":"KwqQL6leFFjR","outputId":"561ec1d3-3418-404d-b316-657dbc31e305"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJvvcHghZJ0b"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"visualize_ViT.ipynb","provenance":[],"mount_file_id":"16RkPv7Q9plSR7wXdAgYyUOhVP88pm8xj","authorship_tag":"ABX9TyOtOC0zc6U0kXtJu8cyFvDH"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}