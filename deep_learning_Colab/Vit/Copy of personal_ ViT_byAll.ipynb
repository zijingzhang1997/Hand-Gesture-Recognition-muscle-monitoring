{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of personal_ ViT_byAll.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZII5pk69M1cG"},"source":["import os\n","import numpy as np\n","import matplotlib\n","import torch\n","# !pip install mat73\n","# import mat73\n","import matplotlib.pyplot as plt\n","import csv\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split,TensorDataset\n","from torchvision import transforms, utils\n","import time\n","import pandas as pd\n","import scipy.io\n","import sklearn.metrics\n","import seaborn as sns\n","import random\n","from sklearn.model_selection import train_test_split\n","from torchvision.transforms import ToTensor\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import accuracy_score\n","random.seed(1)\n","torch.manual_seed(1)\n","torch.cuda.manual_seed(1)\n","np.random.seed(1)\n","\n","from scipy import signal\n","\n","from sklearn.metrics import confusion_matrix\n","\n","import seaborn as sn\n","import pandas as pd\n","\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from sklearn.model_selection import StratifiedGroupKFold\n","from sklearn.model_selection import LeaveOneGroupOut\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from math import sqrt\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","!pip install einops\n","from einops import rearrange, repeat\n","from einops.layers.torch import Rearrange\n","\n","# helpers\n","\n","def pair(t):\n","    return t if isinstance(t, tuple) else (t, t)\n","\n","# classes\n","\n","class PreNorm(nn.Module):\n","    def __init__(self, dim, fn):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fn = fn\n","    def forward(self, x, **kwargs):\n","        return self.fn(self.norm(x), **kwargs)\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim, hidden_dim, dropout = 0.):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class LSA(nn.Module):\n","    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n","        super().__init__()\n","        inner_dim = dim_head *  heads\n","        self.heads = heads\n","        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n","\n","        self.attend = nn.Softmax(dim = -1)\n","        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n","\n","        self.to_out = nn.Sequential(\n","            nn.Linear(inner_dim, dim),\n","            nn.Dropout(dropout)\n","        )\n","\n","    def forward(self, x):\n","        qkv = self.to_qkv(x).chunk(3, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n","\n","        dots = torch.matmul(q, k.transpose(-1, -2)) * self.temperature.exp()\n","\n","        mask = torch.eye(dots.shape[-1], device = dots.device, dtype = torch.bool)\n","        mask_value = -torch.finfo(dots.dtype).max\n","        dots = dots.masked_fill(mask, mask_value)\n","\n","        attn = self.attend(dots)\n","\n","        out = torch.matmul(attn, v)\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        return self.to_out(out)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n","        super().__init__()\n","        self.layers = nn.ModuleList([])\n","        for _ in range(depth):\n","            self.layers.append(nn.ModuleList([\n","                PreNorm(dim, LSA(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n","                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n","            ]))\n","    def forward(self, x):\n","        for attn, ff in self.layers:\n","            x = attn(x) + x\n","            x = ff(x) + x\n","        return x\n","\n","class SPT(nn.Module):\n","    def __init__(self, *, dim, patch_size, channels = 3):\n","        super().__init__()\n","        patch_dim = patch_size * patch_size * 5 * channels\n","\n","        self.to_patch_tokens = nn.Sequential(\n","            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n","            nn.LayerNorm(patch_dim),\n","            nn.Linear(patch_dim, dim)\n","        )\n","\n","    def forward(self, x):\n","        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n","        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n","        x_with_shifts = torch.cat((x, *shifted_x), dim = 1)\n","        return self.to_patch_tokens(x_with_shifts)\n","\n","class ViT(nn.Module):\n","    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n","        super().__init__()\n","        image_height, image_width = pair(image_size)\n","        patch_height, patch_width = pair(patch_size)\n","\n","        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n","\n","        num_patches = (image_height // patch_height) * (image_width // patch_width)\n","        patch_dim = channels * patch_height * patch_width\n","        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n","\n","        self.to_patch_embedding = SPT(dim = dim, patch_size = patch_size, channels = channels)\n","\n","        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n","        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n","        self.dropout = nn.Dropout(emb_dropout)\n","\n","        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n","\n","        self.pool = pool\n","        self.to_latent = nn.Identity()\n","\n","        self.mlp_head = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, num_classes)\n","        )\n","\n","    def forward(self, img):\n","        x = self.to_patch_embedding(img)\n","        b, n, _ = x.shape\n","\n","        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x += self.pos_embedding[:, :(n + 1)]\n","        x = self.dropout(x)\n","\n","        x = self.transformer(x)\n","\n","        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n","\n","        x = self.to_latent(x)\n","        return self.mlp_head(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbbJyKP4Y8zU","executionInfo":{"status":"ok","timestamp":1647876450550,"user_tz":240,"elapsed":2553,"user":{"displayName":"Zijing Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOGqelSoQ8DBz00Gsbqr9QBAymTf0Nlug7AzpX=s64","userId":"03111893666314698246"}},"outputId":"e1c955cc-1a04-45a5-ac85-5e977679293c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n"]}]},{"cell_type":"code","source":["\n","\n","def plotfigFullClass(cm,acc,tle,ExpVer):\n","\n","    parent_dir = r\"/content/drive/My Drive/Colab Notebooks/RFMG/data/\"+ExpVer+\"/result/\"\n","    path1 = os.path.join(parent_dir, 'cm_fig')\n","    os.makedirs(path1, exist_ok = True) \n","    path1 = os.path.join(parent_dir, 'npyFile')\n","    os.makedirs(path1, exist_ok = True) \n","\n","\n","\n","    name1=[\"R\", \"G\",\"G*2\",\"P1\",\"P1*2\",\"P2\",\"P2*2\",\"P23\",\"P23*2\",\"P4\",\"P4*2\",\"sG\",\"sF\",\"sP1\",\"sP2\",\"sP23\",\"sP4\",\"U\",\"U*2\",\"D\",\"D*2\",\"sU\",\"sD\"]\n","    name2=name1\n","    w=20 #fig size 1      change when label num change\n","    h=22   #fig size 2\n","    \n","    # true_num=0\n","    # for i in range (len(cm)):\n","    #   true_num=true_num+ cm[i,i]\n","    # acc=true_num/np.sum(cm)   \n","    \n","    df_cm = pd.DataFrame(cm, index=name1, columns=name2)\n","    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n","    plt.figure()\n","    sn.set(font_scale=1.2)\n","    mask = np.zeros_like(df_cm)\n","    mask[np.where(cm==0)] = True\n","    \n","    sn.heatmap(df_cm, cmap=\"YlGnBu\",fmt=\".2f\",vmin=0, vmax=1.0,annot=True,mask=mask,square=True,cbar=False,annot_kws={'fontsize':15})\n"," \n","    s01=' Acc={n:.3f}'.format(n=acc)\n","    #plt.text(a,b,s01,fontsize=size)\n","    \n","    tleSave=tle\n","    plt.title(tle+s01)\n","\n","    figure = plt.gcf()\n","    figure.set_size_inches(w, h)\n","    \n","    plt.savefig(parent_dir+r\"cm_fig/\"+tleSave+\"CM.png\", dpi=300)\n","\n","\n"],"metadata":{"id":"CXlRIaYjoQC_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["in_ch=48\n","channel=np.linspace(0,47,48).astype(int) \n","#self_channel=[0,1,10,11,20,21,30,31]\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","# in_ch=2\n","# channel=[48,49]\n","num_class=23\n","k=7\n","\n","\n","file_name_data=r\"/content/drive/My Drive/Colab Notebooks/RFMG/data/\"+'2_5'+\"/data_all_multi6_filt_0.1_5.npz\"\n","data=np.load(file_name_data)\n","feature_2d_stft=data['feature_2d_stft']\n","feature_2d_stft2=data['feature_2d_stft2']\n","feature_2d_cwt=data['feature_2d_cwt']\n","feature_2d_cwt2=data['feature_2d_cwt2']\n","feature_2d_cwt3=data['feature_2d_cwt3']\n","\n","label_all=data['label_all']\n","caseNum_all=data['caseNum_all']\n","\n","\n","opt1={'image_size':56,'patch_size':7}\n","\n","opt_list=[opt1]\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"U-6GaGHTJNdD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9869b7a-4826-4f3e-febb-ea3bf6f171d0","executionInfo":{"status":"ok","timestamp":1647876458380,"user_tz":240,"elapsed":7833,"user":{"displayName":"Zijing Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOGqelSoQ8DBz00Gsbqr9QBAymTf0Nlug7AzpX=s64","userId":"03111893666314698246"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# in_ch: CNN input channel number \n","# num_class: classification number  8,5,3 \n","# opt CV method. groupKfold: kfold, all cases not been seen in training set  \n","# Kfold : randomly kfold. label and cases equally distributed\n","# group: leave one case out \n","\n","par_vit={'dim':512,'depth':6,'heads':16,'mlp_dim':64,'dropout':0.1,'emb_dropout':0.1,\\\n","         'n_epoch':25,'learning_rate':1e-4}; \n","par_vit={'dim':512,'depth':6,'heads':16,'mlp_dim':64,'dropout':0.1,'emb_dropout':0.1,\\\n","         'n_epoch':20,'learning_rate':1e-4};   \n","parameter_vit=np.array(list(par_vit.items()));\n","from google.colab import drive\n","drive.mount('/content/drive')\n","X_all=feature_2d_stft\n","y_all=label_all\n","opt=opt1\n","\n","if __name__ == '__main__':   \n","      \n","    cm_all=[]\n","    acc_all=[]\n","    y_test_all=[]\n","    y_p_all=[]\n","    test_ind_all=[]\n","\n","    start_timeall=time.time()\n","    training_time=[]\n"," \n","\n","\n"," \n","\n","    X_train=X_all[:-200,:,:] \n","    X_test=X_all[-200:,:,:]\n","    y_train=y_all[:-200] \n","    y_test=y_all[-200:]\n","    \n","    \n","\n","        \n","\n","  \n","\n","    batchsize_train = 16\n","    train_loader = DataLoader(TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train)), batch_size=batchsize_train,shuffle=True)\n","    batchsize_test = 200\n","    test_loader = DataLoader(TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test)), batch_size=batchsize_test, shuffle=False)\n","            \n","    \n","    n_epoch=par_vit['n_epoch']\n","    #n_epoch=25\n","    learning_rate = par_vit['learning_rate']  #学习的速率 越小越精细\n","    momentum = 0.1\n","    random_seed=1\n","    torch.backends.cudnn.enabled = False\n","    torch.manual_seed(random_seed)     #设定随机数种子为固定值\n","    \n","    train_loss_epoch = []\n","    test_acc_epoch =[]\n","    total_acc_epoch = []\n","    #epoch_range = np.arange(25,225,25)\n","    epoch_range = np.array([2])\n","\n","    \n","    \n","    network = ViT(\n","    image_size = opt['image_size'],\n","    patch_size = opt['patch_size'],\n","    num_classes =num_class,\n","    dim = par_vit['dim'],\n","    depth = par_vit['depth'],\n","    heads = par_vit['heads'],\n","    mlp_dim = par_vit['mlp_dim'],\n","    dropout = par_vit['dropout'],\n","    emb_dropout = par_vit['emb_dropout'],\n","    channels = in_ch\n","    ).cuda()\n","    #training\n","    optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n","    \n","    \n","    \n","    #network.train()\n","    Training_Loss = []\n","    Test_Loss = []\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    for epoch in range(n_epoch):   # loop over the dataset multiple times\n","        train_loss = 0\n","        for X, Y in train_loader:\n","            \n","            X = X.float().cuda()  \n","            Y = Y.long().view(-1, ).cuda() \n","            current_batchsize = X.shape[0]\n","           \n","            optimizer.zero_grad()\n","            output = network(X)\n","            loss = criterion(output,Y)\n","            train_loss = train_loss + loss.item()\n","            loss.backward()                     #calculate the gradient decent\n","            optimizer.step()                    #update the weight\n","            \n","\n","        test_loss = 0\n","        correct = 0\n","        total = 0\n","        test_y= []\n","        test_y_p = []\n","        \n","        # sens = 0    # sensitivity = TP/(TP + FN)\n","        # prec = 0    # precision = TP/(TP + FP)\n","        with torch.no_grad():        \n","            # X = X.view(-1,X.shape[1],X.shape[2])\n","            # X = X.float()         \n","            for X, Y in test_loader:\n","                X = torch.Tensor(X).cuda()\n","                Y = torch.Tensor(Y).long().view(-1, ).cuda()\n","                \n","                images, labels = X, Y\n","                # calculate outputs by running images through the network\n","                start_time=time.time()\n","                outputs = network(images)\n","                \n","                \n","                loss = criterion(outputs,Y)\n","                test_loss = test_loss + loss.item()\n","\n","                # the class with the highest energy is what we choose as prediction\n","                _, predicted = torch.max(outputs.data, 1)\n","                training_time.append(time.time()-start_time)\n","                total += labels.size(0)\n","           \n","                \n","                for i in range(len(labels)):\n","                  test_y.append(labels[i])\n","                  test_y_p.append(predicted[i])\n","                  \n","\n","        Training_Loss.append(train_loss/len(train_loader.dataset))\n","        Test_Loss.append(test_loss/len(test_loader.dataset))\n","        # if epoch%(n_epoch-1)==0:\n","        #     print(train_loss/len(train_loader.dataset))\n","        #     print(test_loss/len(test_loader.dataset))\n","  \n","    train_loss_epoch.append(Training_Loss[-1])\n","    test_y_p=torch.FloatTensor(test_y_p)\n","    test_y=torch.FloatTensor(test_y)\n","  \n","    y_p=np.array(test_y_p.cpu())\n","    y=np.array(test_y.cpu())\n","    \n","\n","    cm=confusion_matrix(y, y_p)\n","    acc=accuracy_score(y, y_p)\n","    \n","    cm_all.append(cm)\n","    acc_all.append(acc)\n","    for i in range(len(y_p)):\n","        y_p_all.append(y_p[i])\n","        y_test_all.append(y_test[i])\n","    print(acc) \n","\n","\n","\n","acc_all=np.array(acc_all)\n","cm_all=np.array(cm_all)\n","y_test_all=np.array(y_test_all)\n","y_p_all=np.array(y_p_all)\n","y=np.stack((y_test_all, y_p_all))\n","\n","test_ind_all=np.array(test_ind_all)\n","\n","cm=confusion_matrix(y_test_all, y_p_all)\n","cm_norm=confusion_matrix(y_test_all, y_p_all,normalize='true')\n","\n","acc=accuracy_score(y_test_all, y_p_all)\n","\n","x_timeall=time.time()-start_timeall\n","print(np.mean(np.array(training_time)))\n"],"metadata":{"id":"YPNhRXsptyXP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647876534586,"user_tz":240,"elapsed":28044,"user":{"displayName":"Zijing Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOGqelSoQ8DBz00Gsbqr9QBAymTf0Nlug7AzpX=s64","userId":"03111893666314698246"}},"outputId":"333de040-28ee-4989-89f3-ee9045e354cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","0.295\n","0.1266635537147522\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjZhMf-UFwnt","executionInfo":{"status":"ok","timestamp":1647876591902,"user_tz":240,"elapsed":241,"user":{"displayName":"Zijing Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhOGqelSoQ8DBz00Gsbqr9QBAymTf0Nlug7AzpX=s64","userId":"03111893666314698246"}},"outputId":"232912ae-d39d-4f85-8bed-477b39894465"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Mar 21 15:29:51 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    33W / 250W |   4593MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]}]}